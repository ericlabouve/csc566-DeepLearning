{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pIilF79ZjANb"
   },
   "source": [
    "# Homework 4: PixelCNN\n",
    "\n",
    "In this homework you will implement a simplified version of the [PixelCNN](https://arxiv.org/pdf/1601.06759.pdf) network and train it on zero digits from the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sh1JAoiDwZIr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.rc('image', cmap='gray')\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ra9PxG6rjbvG"
   },
   "source": [
    "Here we load the MNIST dataset, select only zero digits, and \"binarize\" them so that the pixels are strictly ones and zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cq4V6jwLwByO"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# select only zero digits\n",
    "x_train = x_train[y_train==0]\n",
    "x_test = x_test[y_test==0]\n",
    "\n",
    "# binarize the data\n",
    "def binarize(x):\n",
    "  return x > 0.5\n",
    "\n",
    "x_train = binarize(x_train)\n",
    "x_test = binarize(x_test)\n",
    "\n",
    "# add a fourth dimension for use in Keras\n",
    "x_train = np.expand_dims(x_train,axis=-1)\n",
    "x_test = np.expand_dims(x_test,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JKZMxojWwM2Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binarized mnist images:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAABcCAYAAABz9T77AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAA0FJREFUeJzt3dtyokAUBVCYmv//ZechZZkKZuTSQO/Taz2bRI/NdtOFZH48HhMAuf7c/QQAOEaQA4QT5ADhBDlAOEEOEE6QA4QT5ADhBDlAOEEOEO7vlX9snuchvkb6eDzmtY81kyUzec9clszki0YOEE6QA4QT5ADhBDlAOEEOEE6QA4QT5ADhLr2O/E5r/xPSPG+6tLeUTzMaeTajc/z0TSMHCFe+kfufpJ+tndG7x1VpYFvXSZXXzXots6T1+tHIAcKVbeR7Pz0rt86fnK3sV32dWBvnzuD5u1utGY0cIFyZRq5B/M5slszkHK2b5h0S14ZGDhBOkAOEK7O1wlLiKSI1fF97vW+zVDhONHKAcPGNvMKnaWtmwlbWTDaNHCBcbCNv1SB+7t+9+70pl1RpVb87+hV8s92vx+On2vupkQOEi2vkbm60tPUWo2tvV1uttawxwnqZpjHf21Z6PD40coBwEY28p0++XpjJNarNudUZbepctjzvT2dnPc1GIwcIF9HI1xhlb3OLtTP57XE97gUCSxo5QLj4Rj5aE/9fOz7awKlpzxmVNZJFIwcI13Ujtzf7cscsKsx/7TXze1Vrrme8nh5m1PJqlbU/f+Xxo5EDhOu6kV+hQutk6ej7Ouq6aHVflB5a+FaJz/lJIwcIF9vIj356XrlnlmbUNjpN2+9bM4qR10QCjRwgXGwj3+vsqxiSOUuh2t74lWcSd561aOQA4YZp5Jr4knu7v9gD/rJ3Dr2ujV7vF9R6Xho5QLjYRt7bJ2wPzppJr22rBVepcFQPWaSRA4SLbeRHpTWsK/f60mbDfSqtlbOPrTNnpZEDhBPkAOGG2Vqpcgp45hZLlRlxHmtkvStnpZEDhOu6kbdon1UbxLvX5Qs+L3vXTsWZfH9N1b7ws1b1iwU0coBwXTfyp/Q2cBVzWur1K9p3sUZq0sgBwkU0cjhKE2Wa6q4DjRwgnCAHCCfIAcIJcoBwghwg3Oz6WoBsGjlAOEEOEE6QA4QT5ADhBDlAOEEOEE6QA4QT5ADhBDlAOEEOEE6QA4QT5ADhBDlAOEEOEE6QA4QT5ADhBDlAOEEOEE6QA4QT5ADhBDlAOEEOEE6QA4T7B5qfHNxiPtsPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_as_image(x):\n",
    "  \"\"\" Convenience function to properly show an image. \"\"\"\n",
    "  plt.imshow(np.squeeze(x),cmap='gray')\n",
    "  plt.axis('off')\n",
    "\n",
    "def show_grid_of_images(x):\n",
    "  \"\"\" Convenience function to show five images in a row. \"\"\"\n",
    "  for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    show_as_image(x[i])\n",
    "  plt.show()\n",
    "  \n",
    "print('binarized mnist images:')\n",
    "show_grid_of_images(x_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ARr7u1XZj5ZG"
   },
   "source": [
    "### PixelCNN implementation\n",
    "\n",
    "Here is where you will implement the [PixelCNN](https://arxiv.org/pdf/1601.06759.pdf) model.\n",
    "\n",
    "The concept of the PixelCNN is to create a convolutoinal neural network that outputs a probability distribution **at each pixel** conditioned on all the pixels that precede it.  We consider the pixels in the image to be in row-major order (left-to-right, top-to-bottom).\n",
    "\n",
    "So, the network outputs\n",
    "\n",
    "$$p(x_i|x_1,\\ldots,x_{i-1})$$\n",
    "\n",
    "for each pixel $x_i$.  The predictions are done in parallel; the network takes as input the entire image $x_1,\\ldots,x_n$ and outputs $p(x_i|x_1,\\ldots,x_{i-1})$ at each location $i$.\n",
    "\n",
    "If we used a traditional CNN with convolutional filter kernels, the network output at location $i$ would be able to see pixels \"in the future,\" that is, pixels $x_i,\\ldots,x_n$.  To prevent this, we need to **mask** the filter kernels so that they cannot see pixels $x_i,\\ldots,x_n$.\n",
    "\n",
    "The PixelCNN replaces the typical 2D convolution with a **masked** convolution that is not allowed to look at pixels **after** the center pixel under the filter window.  The very first convolution in the network should not look at the center pixel; later convolutions are allowed to look at the center pixel.\n",
    "\n",
    "The masked convolution is implemented by multiplying the filter kernel with a mask of ones and zeros.  For a $5\\times5$ filter kernel, the mask for the first convolution would be:\n",
    "\n",
    "1 1 1 1 1\n",
    "\n",
    "1 1 1 1 1\n",
    "\n",
    "1 1 0 0 0\n",
    "\n",
    "0 0 0 0 0\n",
    "\n",
    "0 0 0 0 0\n",
    "\n",
    "and the mask for the remaining convolutions would be:\n",
    "\n",
    "1 1 1 1 1\n",
    "\n",
    "1 1 1 1 1\n",
    "\n",
    "1 1 1 0 0\n",
    "\n",
    "0 0 0 0 0\n",
    "\n",
    "0 0 0 0 0\n",
    "\n",
    "The architecture should have the following pattern:\n",
    "\n",
    "* Masked Conv2D with center pixel masking, 7x7 filter, K outputs + ReLU activation\n",
    "* Repeat N times:\n",
    "Masked Conv2D without center pixel masking, 5x5 filter, K outputs + ReLU activation\n",
    "* Conv2D, 1x1 filter, 1 output + sigmoid activation\n",
    "\n",
    "Use 'same' padding for all convolutions.  The image size (28x28) should not change throughout the network.\n",
    "\n",
    "Note that the final 1x1 convolution does not need to be masked, since it only looks at the center pixel.\n",
    "\n",
    "I used N=12, K=32 but you are welcome to experiment with other settings.  The training took ~5 minutes per epoch for me.\n",
    "\n",
    "I provided a partial implementation of the MaskedConv2D layer definition; you need to fill in the part that creates the mask.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FeS9U8gXwXik"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_24 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "masked_conv2d_34 (MaskedConv (None, 28, 28, 32)        1600      \n",
      "_________________________________________________________________\n",
      "masked_conv2d_35 (MaskedConv (None, 28, 28, 32)        25632     \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 28, 28, 1)         33        \n",
      "=================================================================\n",
      "Total params: 27,265\n",
      "Trainable params: 27,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, ZeroPadding2D, Cropping2D, Concatenate, Add, Activation, Lambda\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class MaskedConv2D(Conv2D):\n",
    "  \"\"\" Masked 2D Convolution layer based on Conv2D from Keras \"\"\"\n",
    "  \n",
    "  def __init__(self, mask_center_pixel, **kwargs):\n",
    "    \"\"\" If mask_center_pixel is True, center pixel will be masked out\n",
    "        so that the filter cannot see it.\n",
    "    \"\"\"\n",
    "    super(MaskedConv2D, self).__init__(**kwargs)\n",
    "    self.mask_center_pixel = mask_center_pixel\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # create a mask which will be multiplied with the filter kernel below\n",
    "    #\n",
    "    # the kernel has size (kernel_height,kernel_width,channels_in,channels_out)\n",
    "    #\n",
    "    # we will make a mask with size (kernel_height,kernel_width,1,1)\n",
    "    # so that when we multiply\n",
    "    #\n",
    "    # masked_kernel = self.kernel * mask\n",
    "    #\n",
    "    # the mask will be applied across all input and output channels\n",
    "    #\n",
    "    # note that self.kernel_size = (kernel_height,kernel_width)\n",
    "    \n",
    "    mask = np.ones(self.kernel_size + (1,1))\n",
    "    width = self.kernel_size[0]\n",
    "    height = self.kernel_size[1]\n",
    "    c_x = width // 2\n",
    "    c_y = height // 2\n",
    "    \n",
    "    # Zero out center pixel if masked is True\n",
    "    if self.mask_center_pixel:\n",
    "        mask[c_x,c_y] = 0\n",
    "    \n",
    "    # Zero out the remaining pixels in the middle row\n",
    "    for y in range(c_y + 1, width):\n",
    "        mask[c_x,y] = 0\n",
    "    \n",
    "    # Zero out remaining rows\n",
    "    for x in range(c_y + 1, width):\n",
    "        for y in range(height):\n",
    "            mask[x,y] = 0\n",
    "        \n",
    "\n",
    "    # apply the mask to the kernel\n",
    "    masked_kernel = self.kernel * mask\n",
    "    \n",
    "    # run the convolution operation\n",
    "    outputs = K.conv2d(\n",
    "      inputs,\n",
    "      masked_kernel,\n",
    "      strides=self.strides,\n",
    "      padding='same',\n",
    "      data_format=self.data_format,\n",
    "      dilation_rate=self.dilation_rate)\n",
    "\n",
    "    # add bias\n",
    "    if self.use_bias:\n",
    "      outputs = K.bias_add(\n",
    "        outputs,\n",
    "        self.bias,\n",
    "        data_format=self.data_format)\n",
    "\n",
    "    # apply activation\n",
    "    if self.activation is not None:\n",
    "      return self.activation(outputs)\n",
    "    \n",
    "    # return result\n",
    "    return outputs\n",
    "\n",
    "# create the model here\n",
    "# you can create a masked conv 2D layer with x = MaskedConv2D(...)(x)\n",
    "# just like the normal Keras layers\n",
    "# filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
    "# kernel_size: An integer or tuple/list of a single integer, specifying the length of the 1D convolution window.\n",
    "inputs = Input((28,28,1))\n",
    "x = MaskedConv2D(True, filters=32, kernel_size=(7,7), activation='relu', padding='same')(inputs)\n",
    "x = MaskedConv2D(False, filters=32, kernel_size=(5,5), activation='relu', padding='same')(x)\n",
    "outputs = Conv2D(filters=1, kernel_size=(1,1), activation='sigmoid', padding='same')(x)\n",
    "model = Model(inputs=inputs,outputs=outputs)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yBn4fSwDo7kp"
   },
   "source": [
    "### Sampling from the PixelCNN model\n",
    "\n",
    "To sample from the PixelCNN, we need to sample each pixel sequentially in row-major order, starting from the top-left and working towards the bottom-right.\n",
    "\n",
    "So, we will sample the pixels like this:\n",
    "\n",
    "$$x_1 \\sim p(x_1)$$\n",
    "$$x_2 \\sim p(x_2|x_1)$$\n",
    "$$x_3 \\sim p(x_3|x_1,x_2)$$\n",
    "$$\\vdots$$\n",
    "$$x_n \\sim p(x_n|x_1,\\ldots,x_{n-1})$$\n",
    "\n",
    "Remember that the network outputs all of the conditional probabilities at once, but we only need one of the output values at each step.\n",
    "\n",
    "At each step, you will call model.predict(image) to get the predicted probability of each pixel being equal to one.  Then you can select the probability value for the pixel that you need, sample it using the provided sample_bernoulli() function, and write the result into the image.\n",
    "\n",
    "To sample an image from scratch, you will create a matrix of 28x28 zeros as your starting point.\n",
    "\n",
    "To sample the bottom half of a test image, you will use the test image as the starting point and then sample the 14-th row down to the bottom.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3U3Tx4892sqJ"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAA5JJREFUeJzt3MFKK0EURdGU+P+/XG/k4IGkQ2KnK7XXGgpqO9hc8JAec84bsL+vqx8AeA+xQ4TYIULsECF2iPh+5y8bY/jXP5xszjl++7rLDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdoj4vvoBYEdzztN+9hjjqe9z2SFC7BAhdogQO0SIHSLEDhGmN3jCmdPaWVx2iBA7RIgdIsQOEWKHCLFDhNghws4Ov/jEHf2Iyw4RYocIsUOE2CFC7BAhdogQO0TY2UlaeUd/9lXRR1x2iBA7RIgdIsQOEWKHCLFDhNghws7Ox1p5Kz9y1pZ+j8sOEWKHCLFDhNghQuwQIXaIEDtE2NlZlh39b7nsECF2iBA7RIgdIsQOEWKHCNMbl1l5WltxOnuVyw4RYocIsUOE2CFC7BAhdogQO0TY2TnVqlv6jjv6EZcdIsQOEWKHCLFDhNghQuwQIXaIsLNz16o7+e3W3Mpf4bJDhNghQuwQIXaIEDtEiB0ixA4RdvY4O3qHyw4RYocIsUOE2CFC7BAhdogQO0TY2TdnR+eHyw4RYocIsUOE2CFC7BAhdogwvW3gynnNfPY5XHaIEDtEiB0ixA4RYocIsUOE2CHCzr4AH0PlHVx2iBA7RIgdIsQOEWKHCLFDhNghws4eZ0fvcNkhQuwQIXaIEDtEiB0ixA4RYocIO/uDVv7M+T12dH647BAhdogQO0SIHSLEDhFihwjT2wbMazzCZYcIsUOE2CFC7BAhdogQO0SIHSLs7A+yZfPpXHaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghwnvj4+acVz/Cabzr/38uO0SIHSLEDhFihwixQ4TYIcL0trmdp7Ujr/ztO852LjtEiB0ixA4RYocIsUOE2CFC7BBhZ99AeUvncS47RIgdIsQOEWKHCLFDhNghQuwQMWy00OCyQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhH/AFdGQhB/oONdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "\n",
    "def sample_bernoulli(p):\n",
    "    \"\"\" Sample a matrix of binary values given a matrix of probability values \"\"\"\n",
    "    return np.random.uniform(size=p.shape) < p\n",
    "\n",
    "def sample_image_from_scratch(return_image=False):\n",
    "    \"\"\" Samples an image using the model, starting from all zeros \"\"\"\n",
    "    # Blank image to start\n",
    "    input_image = np.zeros((1,28,28,1))\n",
    "    # Loop through every pixel\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            # Compute probabilities from all pixels given the current image\n",
    "            probabilities = model.predict(input_image)\n",
    "            samples = sample_bernoulli(probabilities)\n",
    "            # Obtain the next pixel value and edit the input_image\n",
    "            p_value = samples[0,i,j,0]\n",
    "            input_image[0,i,j,0] = p_value\n",
    "    if return_image:\n",
    "        return input_image\n",
    "    show_as_image(input_image)\n",
    "    plt.show()\n",
    "\n",
    "# sample an image from scratch and show it\n",
    "sample_image_from_scratch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3U3Tx4892sqJ"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAA7VJREFUeJzt3cFOg1AQQFEx/v8v48qFSQPVFni8e87SjVpzM4nTocu6rh/A/D6v/gGAc4gdIsQOEWKHCLFDxNeZ32xZFv/6h4Ot67o8+rrJDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsECF2iDj1I5s537qO+ynZy/Lwk4U5iMkOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0S4Z5/AyDfrjMNkhwixQ4TYIULsECF2iBA7RFi93cCsq7VXfy+Pov4bkx0ixA4RYocIsUOE2CFC7BAhdoiwZ+e29vb09vC/mewQIXaIEDtEiB0ixA4RYocIsUOEPfsAZr1XZywmO0SIHSLEDhFihwixQ4TYIULsEGHPfgJ79Gtsve7FW3eTHSLEDhFihwixQ4TYIULsECF2iLBnf4PyHv2VffWVr1vxs+FNdogQO0SIHSLEDhFihwixQ4TVW9yVK6S9711eaR7BZIcIsUOE2CFC7BAhdogQO0SIHSLs2Sd3x1NMjmGyQ4TYIULsECF2iBA7RIgdIsQOEfbsTxr5tnrWXfqrv9fIf7MrmOwQIXaIEDtEiB0ixA4RYocIsUOEPTv8w94Of8T3PpjsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIJ65Ma+vMtPiYaZMdIsQOEWKHCLFDhNghQuwQIXaIsGe/gREfS8z9mOwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4R79hu48hnnbunnYbJDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSKcuLLp1fPaUU9kR/25jmSyQ4TYIULsECF2iBA7RIgdIsQOEfbsT9ray175qGd4lskOEWKHCLFDhNghQuwQIXaIEDtE2LO/wci30d4DwA+THSLEDhFihwixQ4TYIULsECF2iBA7RIgdIsQOEWKHCLFDhNghQuwQ4cR1cnvnt05gO0x2iBA7RIgdIsQOEWKHCLFDhNghwp6dQ23t8Ud+BPeMTHaIEDtEiB0ixA4RYocIsUOE2CFicc8MDSY7RIgdIsQOEWKHCLFDhNghQuwQIXaIEDtEiB0ixA4RYocIsUOE2CFC7BAhdogQO0SIHSLEDhFihwixQ4TYIULsEPENKfFODiE271sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def complete_bottom_half(index, return_image=False):\n",
    "    \"\"\" Starting with the top half of the i-th test image, samples the bottom half\"\"\"\n",
    "    input_image = np.expand_dims(x_test[index],axis=0)\n",
    "    # To sample the bottom half of the image, loop through each row,col starting at row 14\n",
    "    for i in range(14, 28):\n",
    "        for j in range(28):\n",
    "            # Compute probabilities from all pixels given the current image\n",
    "            probabilities = model.predict(input_image)\n",
    "            samples = sample_bernoulli(probabilities)\n",
    "            # Obtain the next pixel value and edit the input_image\n",
    "            p_value = samples[0,i,j,0]\n",
    "            input_image[0,i,j,0] = p_value\n",
    "    if return_image:\n",
    "        return input_image\n",
    "    show_as_image(input_image)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\"\"\" WHAT DO I DO HERE? \"\"\"\n",
    "def on_epoch_end(epoch, _):\n",
    "    print()\n",
    "\n",
    "    \n",
    "    \n",
    "# complete the bottom half of the first test image and show it\n",
    "complete_bottom_half(0)\n",
    "\n",
    "# this callback will test sampling after every epoch\n",
    "image_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e3eDbxePrapD"
   },
   "source": [
    "### Training the PixelCNN model\n",
    "\n",
    "The PixelCNN is designed to make max-likelihood training simple.  The model's pdf is easy to compute because of the autoregressive nature of the model:\n",
    "\n",
    "$$p(x_1,\\ldots,x_n) = p(x_1)p(x_2|x_1)\\ldots,p(x_n|x_1,\\ldots,x_{n-1})$$\n",
    "\n",
    "$$\\log p(x_1,\\ldots,x_n) = \\sum_i \\log p(x_i|x_1,\\ldots,x_{i-1})$$\n",
    "\n",
    "To compute the negative log likelihood of a training example, we simply run the model with the training example as input and sum up the per-pixel cross entropy loss.\n",
    "\n",
    "Since we have binary data, we will use the binary cross-entropy loss function.\n",
    "\n",
    "There is nothing for you to implement here -- I am just explaining to you what it is doing internally :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2kONjUGdyG-L"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Bz0jbpBMwIt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5744/5923 [============================>.] - ETA: 0s - loss: 2.7267\n",
      "5923/5923 [==============================] - 2s 261us/step - loss: 2.6963\n",
      "Epoch 2/10\n",
      "5808/5923 [============================>.] - ETA: 0s - loss: 1.6825\n",
      "5923/5923 [==============================] - 1s 209us/step - loss: 1.6823\n",
      "Epoch 3/10\n",
      "5824/5923 [============================>.] - ETA: 0s - loss: 1.6365\n",
      "5923/5923 [==============================] - 1s 209us/step - loss: 1.6352\n",
      "Epoch 4/10\n",
      "5776/5923 [============================>.] - ETA: 0s - loss: 1.6120\n",
      "5923/5923 [==============================] - 1s 209us/step - loss: 1.6107\n",
      "Epoch 5/10\n",
      "5904/5923 [============================>.] - ETA: 0s - loss: 1.5950\n",
      "5923/5923 [==============================] - 1s 214us/step - loss: 1.5944\n",
      "Epoch 6/10\n",
      "5696/5923 [===========================>..] - ETA: 0s - loss: 1.5784\n",
      "5923/5923 [==============================] - 1s 201us/step - loss: 1.5779\n",
      "Epoch 7/10\n",
      "5728/5923 [============================>.] - ETA: 0s - loss: 1.5686\n",
      "5923/5923 [==============================] - 1s 200us/step - loss: 1.5674\n",
      "Epoch 8/10\n",
      "5696/5923 [===========================>..] - ETA: 0s - loss: 1.5597\n",
      "5923/5923 [==============================] - 1s 201us/step - loss: 1.5591\n",
      "Epoch 9/10\n",
      "5920/5923 [============================>.] - ETA: 0s - loss: 1.5504\n",
      "5923/5923 [==============================] - 1s 200us/step - loss: 1.5498\n",
      "Epoch 10/10\n",
      "5696/5923 [===========================>..] - ETA: 0s - loss: 1.5451\n",
      "5923/5923 [==============================] - 1s 201us/step - loss: 1.5444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15d02d8e860>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, x_train,\n",
    "          batch_size=16,\n",
    "          epochs=10,\n",
    "          callbacks=[image_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ERb6QcDrsr-Y"
   },
   "source": [
    "### Exercises\n",
    "\n",
    "* Complete the implementation of the masked convolution layer (above)\n",
    "* Implement the PixelCNN model (above)\n",
    "* Implement the sampling functions (above)\n",
    "* Once the model has finished training, sample a large number of images and try to determine whether the network exhibits mode collapse, like GANs sometimes do.  \n",
    "* Critical thinking questions (no code required): Can the PixelCNN auto-complete the **top half** of an image? If not, how would you modify the network to do that?  How could you make it auto-complete the left or right halves?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_25_images(x):\n",
    "    counter = 0\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            plt.subplot(5,5,counter+1)\n",
    "            show_as_image(x[counter])\n",
    "            counter += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Images from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qU7XkoRWs2Hu"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = []\n",
    "for i in range(25):\n",
    "    samples.append(sample_image_from_scratch(return_image=True))\n",
    "show_25_images(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completed Bottom Halves of Testing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = []\n",
    "for i in range(25):\n",
    "    samples.append(complete_bottom_half(i, return_image=True))\n",
    "show_25_images(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is there moad collapse? (Doesn't look like it).\n",
    "\n",
    "#### Can the PixelCNN auto-complete the top/left/right halves of an image? (Just rotate the input image and then run the model but this may not produce results that are consistent with our training data. You can also try changing the filters so that the mask is in the appropriate areas.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "mnist_pixelcnn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
